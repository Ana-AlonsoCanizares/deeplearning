{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ana-AlonsoCanizares/deeplearning/blob/main/Pr%C3%A1ctica2_DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Práctica 2: Clasificar texto con Codificador Transformer"
      ],
      "metadata": {
        "id": "f6vi-j1PQsgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integrantes del grupo**\n",
        "\n",
        "Álvaro García Cid\n",
        "\n",
        "Ana Alonso Cañizares\n",
        "\n",
        "Álvaro García Parra"
      ],
      "metadata": {
        "id": "4wbPPrN4REg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objetivo"
      ],
      "metadata": {
        "id": "h2_tteyoRWDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizar un Codificador Transformer para clasificar las noticias de Reuters en 46 temas mutuamente excluyentes. Como cada noticia debe clasificarse en una sola categoría, es un problema de \"clasificación multiclase de una sola etiqueta\".\n",
        "\n",
        "El Reuters dataset es un conjunto de noticias breves y sus temas, publicado por Reuters en 1986. Son 46 temas diferentes; algunos temas están más representados que otros, pero cada uno tiene, al menos, 10 ejemplos en el conjunto de entrenamiento.\n",
        "\n",
        "Al igual que IMDB y MNIST, el conjunto de datos de Reuters viene empaquetado como parte de Keras."
      ],
      "metadata": {
        "id": "NnuDP4a7YHsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desarrollo"
      ],
      "metadata": {
        "id": "TaRh2Ki9DWR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importación de librerías, funciones y variables"
      ],
      "metadata": {
        "id": "mT-nS3zwDZCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importación de las librerías a utilizar\n",
        "from keras.datasets import reuters\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import kerastuner as kt"
      ],
      "metadata": {
        "id": "PjjS2Nl0GGdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35b0ec2f-3630-4a6c-fa45-1be9a8b13921"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-771e1265936d>:7: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  import kerastuner as kt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lectura de datos"
      ],
      "metadata": {
        "id": "Fg0Ppcl1DfDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000  # Tamaño del vocabulario\n",
        "batch_size = 32     # Tamaño del lote\n",
        "embed_dim = 256     # Dimensión del embedding\n",
        "num_heads = 2       # Número de cabezas del MultiHead\n",
        "dense_dim = 32      # Nº de neuronas de la capa densa\n",
        "EPOCHS = 20         # Nº de épocas"
      ],
      "metadata": {
        "id": "_aJmHh3unogT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HvD3IE7rQnHO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c50a60a5-6905-4a89-a43d-8ee81283c59c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2110848/2110848 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=vocab_size)\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y validación\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula la longitud de cada fila y luego toma la media como la longitud máxima\n",
        "lengths = [len(text) for text in train_data]\n",
        "max_length = round(sum(lengths) / len(lengths)) # Tamaño máximo de tokens\n",
        "\n",
        "# Aplicar relleno o truncado a las secuencias\n",
        "train_data = tf.keras.preprocessing.sequence.pad_sequences(train_data, maxlen=max_length, padding='post')\n",
        "val_data = tf.keras.preprocessing.sequence.pad_sequences(val_data, maxlen=max_length, padding='post')\n",
        "test_data = tf.keras.preprocessing.sequence.pad_sequences(test_data, maxlen=max_length, padding='post')"
      ],
      "metadata": {
        "id": "13mFY1zdzYH5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Tamaño de los vectores de los tokens de entrada\n",
        "        self.embed_dim = embed_dim\n",
        "        # Tamaño de la capa densa interna\n",
        "        self.dense_dim = dense_dim\n",
        "        # Número de attention heads\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    # El cálculo va en call()\n",
        "    def call(self, inputs, mask=None):\n",
        "        # La máscara que generará la capa Embedding\n",
        "        # será 2D, pero la capa de atención espera\n",
        "        # ser 3D o 4D, por lo que ampliamos su rango\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    # Implementamos la serialización para\n",
        "    # que podamos guardar el modelo\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "tUNN-GdcxxUj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    # Una desventaja de las incrustaciones de posición es que\n",
        "    # la longitud de la secuencia debe conocerse de antemano\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Prepara una capa de embedding para los índices de token.\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            # Y otro para las posiciones te tokens\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        # Agrega ambos vectores embeddings juntos\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        # Al igual que la capa de embedding,\n",
        "        # esta capa debería poder generar una\n",
        "        # máscara para que podamos ignorar los\n",
        "        # ceros de relleno en las entradas.\n",
        "        # El framework llamará automáticamente\n",
        "        # al método compute_mask y la máscara\n",
        "        # se propagará a la siguiente capa.\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    # Implementamos la serialización para que\n",
        "    # podamos guardar el modelo.\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        # config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "-HMYrfMgtP6B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos los Dataset\n",
        "# Datos de entrenamiento\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((list(train_data), list(train_labels)))\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)  # Precargamos datos para acelerar el entrenamiento\n",
        "\n",
        "# Datos de validación\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((list(val_data), list(val_labels)))\n",
        "val_ds = val_ds.batch(batch_size)\n",
        "val_ds = val_ds.prefetch(tf.data.experimental.AUTOTUNE)  # Precargamos datos para acelerar el entrenamiento\n",
        "\n",
        "# Datos de evaluación\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((list(test_data), list(test_labels)))\n",
        "test_ds = test_ds.batch(batch_size)\n",
        "test_ds = test_ds.prefetch(tf.data.experimental.AUTOTUNE)  # Precargamos datos para acelerar el entrenamiento"
      ],
      "metadata": {
        "id": "FBUE39NNuXO2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = PositionalEmbedding(max_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(46, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glGCFklv0Su7",
        "outputId": "53336c02-7d3f-4f08-8c6f-1e35282bd62e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " positional_embedding (Posi  (None, None, 256)         2597376   \n",
            " tionalEmbedding)                                                \n",
            "                                                                 \n",
            " transformer_encoder (Trans  (None, None, 256)         543776    \n",
            " formerEncoder)                                                  \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 256)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 46)                11822     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3152974 (12.03 MB)\n",
            "Trainable params: 3152974 (12.03 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)\n",
        "print(f\"Test acc: {model.evaluate(test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I1f3yj82QCx",
        "outputId": "f4cf780b-c3fc-43fd-eac2-a71290554373"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "225/225 [==============================] - 26s 62ms/step - loss: 2.2634 - accuracy: 0.5084 - val_loss: 1.4188 - val_accuracy: 0.6800\n",
            "Epoch 2/20\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.3639 - accuracy: 0.6848 - val_loss: 1.1289 - val_accuracy: 0.7340\n",
            "Epoch 3/20\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0344 - accuracy: 0.7626 - val_loss: 1.0001 - val_accuracy: 0.7596\n",
            "Epoch 4/20\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8207 - accuracy: 0.7985 - val_loss: 1.0094 - val_accuracy: 0.7774\n",
            "Epoch 5/20\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.6771 - accuracy: 0.8344 - val_loss: 1.0416 - val_accuracy: 0.7696\n",
            "Epoch 6/20\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5701 - accuracy: 0.8611 - val_loss: 1.0161 - val_accuracy: 0.7802\n",
            "Epoch 7/20\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4727 - accuracy: 0.8809 - val_loss: 0.9773 - val_accuracy: 0.7880\n",
            "Epoch 8/20\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 0.3977 - accuracy: 0.8983 - val_loss: 1.0852 - val_accuracy: 0.7891\n",
            "Epoch 9/20\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3357 - accuracy: 0.9168 - val_loss: 1.0824 - val_accuracy: 0.7952\n",
            "Epoch 10/20\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.3050 - accuracy: 0.9196 - val_loss: 1.0767 - val_accuracy: 0.7741\n",
            "Epoch 11/20\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.2756 - accuracy: 0.9286 - val_loss: 1.2261 - val_accuracy: 0.7707\n",
            "Epoch 12/20\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2505 - accuracy: 0.9347 - val_loss: 1.2506 - val_accuracy: 0.7718\n",
            "Epoch 13/20\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2259 - accuracy: 0.9417 - val_loss: 1.1911 - val_accuracy: 0.7546\n",
            "Epoch 14/20\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.2126 - accuracy: 0.9434 - val_loss: 1.2410 - val_accuracy: 0.7858\n",
            "Epoch 15/20\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1973 - accuracy: 0.9496 - val_loss: 1.2985 - val_accuracy: 0.7863\n",
            "Epoch 16/20\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1815 - accuracy: 0.9518 - val_loss: 1.4281 - val_accuracy: 0.7752\n",
            "Epoch 17/20\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1736 - accuracy: 0.9517 - val_loss: 1.2846 - val_accuracy: 0.7774\n",
            "Epoch 18/20\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1665 - accuracy: 0.9514 - val_loss: 1.2924 - val_accuracy: 0.7913\n",
            "Epoch 19/20\n",
            "225/225 [==============================] - 4s 17ms/step - loss: 0.1537 - accuracy: 0.9557 - val_loss: 1.4467 - val_accuracy: 0.7657\n",
            "Epoch 20/20\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.1501 - accuracy: 0.9564 - val_loss: 1.3746 - val_accuracy: 0.7679\n",
            "71/71 [==============================] - 0s 7ms/step - loss: 1.5440 - accuracy: 0.7395\n",
            "Test acc: 0.740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = PositionalEmbedding(max_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(46, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"Adam\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3kDCnUqdB4i",
        "outputId": "e827b370-36b1-4efd-e7f5-5c4a901b9e6e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " positional_embedding_1 (Po  (None, None, 256)         2597376   \n",
            " sitionalEmbedding)                                              \n",
            "                                                                 \n",
            " transformer_encoder_1 (Tra  (None, None, 256)         543776    \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Gl  (None, 256)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 46)                11822     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3152974 (12.03 MB)\n",
            "Trainable params: 3152974 (12.03 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)\n",
        "print(f\"Test acc: {model.evaluate(test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZWLilvudClS",
        "outputId": "2a64d104-42c8-4742-eb2c-cee39a912fe5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "225/225 [==============================] - 24s 89ms/step - loss: 2.3411 - accuracy: 0.4910 - val_loss: 1.4054 - val_accuracy: 0.6923\n",
            "Epoch 2/20\n",
            "225/225 [==============================] - 4s 17ms/step - loss: 1.2517 - accuracy: 0.7203 - val_loss: 1.0224 - val_accuracy: 0.7702\n",
            "Epoch 3/20\n",
            "225/225 [==============================] - 4s 17ms/step - loss: 0.7804 - accuracy: 0.8117 - val_loss: 0.9971 - val_accuracy: 0.7902\n",
            "Epoch 4/20\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.5315 - accuracy: 0.8661 - val_loss: 1.0543 - val_accuracy: 0.7774\n",
            "Epoch 5/20\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.3699 - accuracy: 0.9044 - val_loss: 0.9815 - val_accuracy: 0.8002\n",
            "Epoch 6/20\n",
            "225/225 [==============================] - 4s 17ms/step - loss: 0.2645 - accuracy: 0.9303 - val_loss: 1.1139 - val_accuracy: 0.7980\n",
            "Epoch 7/20\n",
            "225/225 [==============================] - 4s 17ms/step - loss: 0.2069 - accuracy: 0.9422 - val_loss: 1.3119 - val_accuracy: 0.7841\n",
            "Epoch 8/20\n",
            "225/225 [==============================] - 4s 18ms/step - loss: 0.1712 - accuracy: 0.9468 - val_loss: 1.3432 - val_accuracy: 0.7830\n",
            "Epoch 9/20\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.1591 - accuracy: 0.9493 - val_loss: 1.4011 - val_accuracy: 0.7757\n",
            "Epoch 10/20\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.1429 - accuracy: 0.9537 - val_loss: 1.3594 - val_accuracy: 0.7807\n",
            "Epoch 11/20\n",
            "225/225 [==============================] - 4s 17ms/step - loss: 0.1240 - accuracy: 0.9566 - val_loss: 1.4138 - val_accuracy: 0.7819\n",
            "Epoch 12/20\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.1210 - accuracy: 0.9576 - val_loss: 1.4364 - val_accuracy: 0.7852\n",
            "Epoch 13/20\n",
            "225/225 [==============================] - 4s 18ms/step - loss: 0.1138 - accuracy: 0.9594 - val_loss: 1.4868 - val_accuracy: 0.7824\n",
            "Epoch 14/20\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.1085 - accuracy: 0.9601 - val_loss: 1.5161 - val_accuracy: 0.7774\n",
            "Epoch 15/20\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.1081 - accuracy: 0.9570 - val_loss: 1.6028 - val_accuracy: 0.7691\n",
            "Epoch 16/20\n",
            "225/225 [==============================] - 4s 17ms/step - loss: 0.1066 - accuracy: 0.9588 - val_loss: 1.5956 - val_accuracy: 0.7663\n",
            "Epoch 17/20\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.1017 - accuracy: 0.9587 - val_loss: 1.5373 - val_accuracy: 0.7824\n",
            "Epoch 18/20\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.1007 - accuracy: 0.9614 - val_loss: 1.5240 - val_accuracy: 0.7869\n",
            "Epoch 19/20\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.0959 - accuracy: 0.9596 - val_loss: 1.5803 - val_accuracy: 0.7819\n",
            "Epoch 20/20\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.0954 - accuracy: 0.9606 - val_loss: 1.6470 - val_accuracy: 0.7830\n",
            "71/71 [==============================] - 1s 7ms/step - loss: 1.8346 - accuracy: 0.7573\n",
            "Test acc: 0.757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PRUEBA TUNER OPTIMIZACIÓN BAYESIANA"
      ],
      "metadata": {
        "id": "UVTkxRathKp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFVaFn7KhKL9",
        "outputId": "5a843c82-84b9-4ed3-875e-1fbc385ab0c6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "    vocab_size = 10000  # Tamaño fijo del vocabulario\n",
        "    max_length = hp.Int('max_length', min_value=100, max_value=500, step=50)\n",
        "    num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)\n",
        "    embed_dim = hp.Int('embed_dim', min_value=64, max_value=512, step=64)\n",
        "    dense_dim = hp.Int('dense_dim', min_value=32, max_value=128, step=32)\n",
        "\n",
        "    inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "    x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "    x = PositionalEmbedding(max_length, vocab_size, embed_dim)(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(46, activation=\"softmax\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Definimos el tuner\n",
        "tuner = kt.BayesianOptimization(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,  # Número de combinaciones de hiperparámetros a probar\n",
        "    executions_per_trial=1,  # Cuántas veces se ejecutará cada modelo para promediar métricas\n",
        "    directory='my_dir',  # Directorio donde se guardan los logs\n",
        "    project_name='transformer_tuning'\n",
        ")\n",
        "\n",
        "# Callback para detener temprano si no se observan mejoras\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Inicia la búsqueda\n",
        "tuner.search(train_ds, epochs=10, validation_data=val_ds, callbacks=[stop_early])\n",
        "\n",
        "# Obtén el mejor modelo\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Evalúa el mejor modelo\n",
        "loss, accuracy = best_model.evaluate(test_ds)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
        "\n",
        "# Sacamos la mejor combinación lograda\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"\"\"\n",
        "El mejor número de cabezas de atención: {best_hps.get('num_heads')}\n",
        "El mejor tamaño de embedding: {best_hps.get('embed_dim')}\n",
        "La mejor dimensión de la capa densa: {best_hps.get('dense_dim')}\n",
        "El mejor tamaño máximo de secuencia: {best_hps.get('max_length')}\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtc4pfUYhOjx",
        "outputId": "f5407935-cc1b-4a34-bfae-9c7d3cc3270f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 01m 37s]\n",
            "val_accuracy: 0.7885364294052124\n",
            "\n",
            "Best val_accuracy So Far: 0.8007791042327881\n",
            "Total elapsed time: 00h 18m 45s\n",
            "71/71 [==============================] - 1s 10ms/step - loss: 1.1698 - accuracy: 0.7685\n",
            "Test Loss: 1.1698168516159058, Test Accuracy: 0.7684773206710815\n",
            "\n",
            "El mejor número de cabezas de atención: 6\n",
            "El mejor tamaño de embedding: 192\n",
            "La mejor dimensión de la capa densa: 128\n",
            "El mejor tamaño máximo de secuencia: 100\n",
            "\n"
          ]
        }
      ]
    }
  ]
}